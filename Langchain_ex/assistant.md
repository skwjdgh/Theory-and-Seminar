# whisper_module.py
import io
import subprocess
import whisper
import streamlit as st

class WhisperRecognizer:
    def __init__(self):
        self.model = self.load_model()

    @st.cache_resource
    def load_model(self):
        return whisper.load_model("base")

    def transcode_to_wav(self, audio_bytes: bytes) -> io.BytesIO:
        temp_in = '/tmp/input_audio'
        temp_out = '/tmp/output.wav'
        with open(temp_in, 'wb') as f:
            f.write(audio_bytes)
        subprocess.run(['ffmpeg', '-y', '-i', temp_in, temp_out], check=False)
        wav_bio = io.BytesIO(open(temp_out, 'rb').read())
        wav_bio.name = 'audio.wav'
        return wav_bio

    def recognize(self, audio_bytes):
        try:
            wav = self.transcode_to_wav(audio_bytes)
            result = self.model.transcribe(wav, language="ko")
            return result.get("text", None)
        except Exception as e:
            st.error(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
            return None


# chain_module.py
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
import streamlit as st

class ChainBuilder:
    def __init__(self):
        self.embeddings = self.load_embeddings()
        self.vectorstore = self.load_vectorstore(self.embeddings)
        self.retriever = self.vectorstore.as_retriever()
        self.llm = ChatOllama(model="llama3.1", base_url="http://localhost:11434", temperature=0.7, timeout_seconds=10)
        self.memory = self.get_memory()

    @st.cache_resource
    def load_embeddings(self):
        return OllamaEmbeddings(model="llama3.1", base_url="http://localhost:11434")

    @st.cache_resource
    def load_vectorstore(self, embeddings):
        docs = [
            Document(page_content="íšŒì˜ëŠ” ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤í›„ 3ì‹œì— ì§„í–‰ë©ë‹ˆë‹¤."),
            Document(page_content="ê°œì¸ ë¹„ì„œëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì¹œì ˆí•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        ]
        return FAISS.from_documents(docs, embeddings)

    def get_memory(self):
        if 'memory' not in st.session_state:
            st.session_state.memory = ConversationBufferMemory(return_messages=True)
        return st.session_state.memory

    def build_base_chain(self):
        prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ì „ë¬¸ ê°œì¸ ë¹„ì„œì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
            1. ì‚¬ìš©ì ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•´ ë‹µë³€
            2. ì¹œì ˆí•œ ë§íˆ¬ ì‚¬ìš© (ì¡´ëŒ“ë§)
            3. ë‹µë³€ì€ ìµœëŒ€ 3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ

            ìš”ì²­: {message}
            ë‹µë³€:"""
        )
        return prompt | self.llm | StrOutputParser()

    def build_rag_chain(self):
        rag_prompt = ChatPromptTemplate.from_template("ë¬¸ë§¥: {context}\nì§ˆë¬¸: {question}\në‹µë³€:")
        return ( {"context": self.retriever, "question": RunnablePassthrough()} | rag_prompt | self.llm | StrOutputParser())

    def build_conversation_chain(self, base_chain):
        return RunnableWithMessageHistory(
            base_chain,
            lambda session_id: self.memory,
            input_messages_key="message",
            history_messages_key="history"
        )


# tool_module.py
from langchain.agents import Tool, initialize_agent, AgentType

class AssistantTools:
    def __init__(self, base_chain, llm, memory):
        self.agent = self.build_agent(base_chain, llm, memory)

    def build_agent(self, base_chain, llm, memory):
        def draft_email_tool(input_text: str) -> str:
            return base_chain.invoke({"message": f"ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ ì´ë©”ì¼ì„ ì‘ì„±í•´ì£¼ì„¸ìš”: {input_text}"})

        def schedule_meeting_tool(input_text: str) -> str:
            return base_chain.invoke({"message": f"{input_text} ì¼ì • ì¡ì•„ì¤˜"})

        def qna_tool(input_text: str) -> str:
            return base_chain.invoke({"message": input_text})

        tools = [
            Tool(name="DraftEmail", func=draft_email_tool, description="ë‚´ìš©ì„ ì…ë ¥í•˜ë©´ ì´ë©”ì¼ì„ ì‘ì„±í•´ë“œë¦½ë‹ˆë‹¤."),
            Tool(name="ScheduleMeeting", func=schedule_meeting_tool, description="íšŒì˜ ì¼ì •ì„ ì¡°ìœ¨í•©ë‹ˆë‹¤."),
            Tool(name="QnA", func=qna_tool, description="ì¼ë°˜ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
        ]
        return initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, memory=memory)


# ui_module.py
import streamlit as st
import threading
from streamlit_mic_recorder import mic_recorder
from whisper_module import WhisperRecognizer

class AssistantUI:
    def __init__(self, agent, rag_chain, conversation_chain):
        self.agent = agent
        self.rag_chain = rag_chain
        self.conversation_chain = conversation_chain
        self.recognizer = WhisperRecognizer()

    def tts_play(self, text: str):
        import pyttsx3
        try:
            engine = pyttsx3.init()
            engine.say(text)
            engine.runAndWait()
        except Exception:
            pass

    def run_ui(self):
        st.title("ğŸ¤– ê°œì¸ìš© AI ë¹„ì„œ ì„œë¹„ìŠ¤")
        st.subheader("í…ìŠ¤íŠ¸/ìŒì„± ì…ë ¥ â†’ Llama 3.1 ê¸°ë°˜ ë‹µë³€")
        st.write("ë§ˆì´í¬ ì‚¬ìš© ì‹œ ë¸Œë¼ìš°ì € ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.")

        input_mode = st.radio("ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”", ("í…ìŠ¤íŠ¸", "ìŒì„±"), horizontal=True)
        input_text = ""

        if input_mode == "í…ìŠ¤íŠ¸":
            input_text = st.text_input("ë¹„ì„œì—ê²Œ í•  ë§ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì˜¤ëŠ˜ ì¼ì • ì•Œë ¤ì¤˜")

        elif input_mode == "ìŒì„±":
            st.write("ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë§ˆì´í¬ë¡œ ë§í•˜ì„¸ìš” ğŸ‘‡")
            audio = mic_recorder(start_prompt="ğŸ¤ ë§í•˜ê¸° ì‹œì‘", stop_prompt="â¹ï¸ ë…¹ìŒ ì¢…ë£Œ", key="recorder")
            if audio:
                st.audio(audio['bytes'], format="audio/webm")
                input_text = self.recognizer.recognize(audio['bytes'])

        if input_text:
            if "ì´ë©”ì¼" in input_text or "ì¼ì •" in input_text:
                response = self.agent.run(input_text)
            elif "íšŒì˜" in input_text or "ì •ë³´" in input_text:
                response = self.rag_chain.invoke({"question": input_text})
            else:
                response = self.conversation_chain.invoke(
                    {"message": input_text},
                    config={"configurable": {"session_id": "user-session"}}
                )

            st.subheader("ğŸ“ ë¹„ì„œ ë‹µë³€")
            st.write(response)

            st.subheader("ğŸ”Š ìŒì„± ì¶œë ¥")
            threading.Thread(target=self.tts_play, args=(response,), daemon=True).start()

            if 'history' not in st.session_state:
                st.session_state.history = []
            st.session_state.history.append(("ğŸ‘¤ ì‚¬ìš©ì", input_text))
            st.session_state.history.append(("ğŸ¤– ë¹„ì„œ", response))
            if len(st.session_state.history) > 50:
                st.session_state.history = st.session_state.history[-50:]

            st.subheader("ğŸ—‚ï¸ ëŒ€í™” ì´ë ¥")
            for role, msg in st.session_state.history:
                st.text(f"{role}: {msg}")


# main.py
from chain_module import ChainBuilder
from tool_module import AssistantTools
from ui_module import AssistantUI

if __name__ == "__main__":
    chain_builder = ChainBuilder()
    base_chain = chain_builder.build_base_chain()
    rag_chain = chain_builder.build_rag_chain()
    conversation_chain = chain_builder.build_conversation_chain(base_chain)

    tools = AssistantTools(base_chain, chain_builder.llm, chain_builder.memory)
    ui = AssistantUI(tools.agent, rag_chain, conversation_chain)
    ui.run_ui()