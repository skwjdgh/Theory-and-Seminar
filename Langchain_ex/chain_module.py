# chain_module.py (ìˆ˜ì • ì™„ë£Œ)
import os
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

load_dotenv()

class ChainBuilder:
    def __init__(self, status_text_area=None):
        self.status_text = status_text_area
        self._update_status("ğŸ“Š ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embeddings = self.load_embeddings()
        
        self._update_status("ğŸ“š ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...")
        self.vectorstore = self.load_vectorstore(self.embeddings)
        self.retriever = self.vectorstore.as_retriever()
        
        self._update_status("ğŸ¤– LLM ì—°ê²° ì¤‘...")
        self.llm = self._create_llm()

    def _update_status(self, text):
        if self.status_text:
            self.status_text.text(text)

    def load_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
        try:
            embeddings = OllamaEmbeddings(
                model=os.getenv("EMBEDDING_MODEL", "llama3"),
                base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            )
            embeddings.embed_query("ì—°ê²° í…ŒìŠ¤íŠ¸")
            return embeddings
        except Exception as e:
            raise RuntimeError(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

    def load_vectorstore(self, _embeddings):
        """ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""
        docs = [
            Document(page_content="ì—…ë°ì´íŠ¸ëœ íšŒì˜ ì •ë³´: ë§¤ì£¼ í™”ìš”ì¼ ì˜¤ì „ 10ì‹œ"),
            Document(page_content="ìƒˆë¡œìš´ ì •ì±…: ëª¨ë“  ìš”ì²­ì€ 24ì‹œê°„ ë‚´ ì²˜ë¦¬"),
            Document(page_content="íšŒì‚¬ ì—°ë½ì²˜: ë³¸ì‚¬ 02-1234-5678, ì§€ì›íŒ€ 02-8765-4321"),
            Document(page_content="ì—…ë¬´ ì‹œê°„: í‰ì¼ ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ"),
        ]
        return FAISS.from_documents(docs, _embeddings)

    def _create_llm(self):
        """LLM ìƒì„± ë° ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            llm = ChatOllama(
                model=os.getenv("LLM_MODEL", "llama3"),
                base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                temperature=0.7,
            )
            llm.invoke("ì—°ê²° í…ŒìŠ¤íŠ¸")
            return llm
        except Exception as e:
            raise RuntimeError(f"LLM ì—°ê²° ì‹¤íŒ¨: {e}")

    def build_base_chain(self):
        prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ì „ë¬¸ ê°œì¸ ë¹„ì„œì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
            1. ì‚¬ìš©ì ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ ì œê³µ
            2. ì¹œì ˆí•˜ê³  ì •ì¤‘í•œ ë§íˆ¬ ì‚¬ìš© (ì¡´ëŒ“ë§)
            3. ë‹µë³€ì€ ê°„ê²°í•˜ë˜ í•„ìš”í•œ ì •ë³´ëŠ” ì¶©ë¶„íˆ í¬í•¨
            4. ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€

            ëŒ€í™” ì´ë ¥: {history}
            í˜„ì¬ ìš”ì²­: {message}
            
            ë‹µë³€:"""
        )
        return prompt | self.llm | StrOutputParser()

    def build_rag_chain(self):
        rag_prompt = ChatPromptTemplate.from_template(
            """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

            ì°¸ê³  ë¬¸ì„œ:
            {context}

            ì§ˆë¬¸: {question}
            
            ë‹µë³€: ì°¸ê³  ë¬¸ì„œì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."""
        )
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        return (
            {"context": self.retriever | format_docs, "question": RunnablePassthrough()}
            | rag_prompt
            | self.llm
            | StrOutputParser()
        )

    def build_conversation_chain(self):
        """ëŒ€í™” ì²´ì¸ ìƒì„± (ê¸°ë¡ ê´€ë¦¬ëŠ” UIì—ì„œ ë‹´ë‹¹)"""
        base_chain = self.build_base_chain()
        
        def conversation_handler(message: str):
            if 'conversation_history' not in st.session_state:
                st.session_state.conversation_history = []
            
            history = st.session_state.conversation_history
            formatted_history = "\n".join([
                f"ì‚¬ìš©ì: {conv['user']}\në¹„ì„œ: {conv['assistant']}"
                for conv in history[-3:]  # ìµœê·¼ 3ê°œì˜ ëŒ€í™”ë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
            ])
            
            try:
                response = base_chain.invoke({
                    "message": message,
                    "history": formatted_history
                })
                return {"response": response, "success": True}
            except Exception as e:
                error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                return {"response": error_msg, "success": False}
        
        return conversation_handler