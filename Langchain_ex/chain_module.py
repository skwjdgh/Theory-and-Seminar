# chain_module.py
import os
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langgraph.graph import StateGraph, START, END
from typing import Dict, Any, List, TypedDict
import streamlit as st
from dotenv import load_dotenv
import time

load_dotenv()

# ìˆ˜ì •: TypedDictë¡œ State ì •ì˜
class ConversationState(TypedDict):
    message: str
    response: str
    success: bool

class ChainBuilder:
    def __init__(self):
        st.write("ğŸ“Š ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embeddings = self.load_embeddings()
        
        st.write("ğŸ“š ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...")
        self.vectorstore = self.load_vectorstore(self.embeddings)
        self.retriever = self.vectorstore.as_retriever()
        
        st.write("ğŸ¤– LLM ì—°ê²° ì¤‘...")
        self.llm = self._create_llm()

    @staticmethod
    def load_embeddings():
        """ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
        try:
            st.write("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            embeddings = OllamaEmbeddings(
                model=os.getenv("EMBEDDING_MODEL", "llama3"),
                base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            )
            
            # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸
            st.write("ğŸ”„ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_embed = embeddings.embed_query("ì•ˆë…•í•˜ì„¸ìš”")
            st.write(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì°¨ì›ìˆ˜: {len(test_embed)})")
            return embeddings
            
        except Exception as e:
            error_msg = str(e)
            if "connection" in error_msg.lower() or "timeout" in error_msg.lower():
                st.error("âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            elif "model" in error_msg.lower():
                st.error(f"âŒ ëª¨ë¸ '{os.getenv('EMBEDDING_MODEL', 'llama3')}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
            else:
                st.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {error_msg}")
            raise e

    @staticmethod
    def load_vectorstore(_embeddings):
        """ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""
        try:
            docs = [
                Document(page_content="ì—…ë°ì´íŠ¸ëœ íšŒì˜ ì •ë³´: ë§¤ì£¼ í™”ìš”ì¼ ì˜¤ì „ 10ì‹œ"),
                Document(page_content="ìƒˆë¡œìš´ ì •ì±…: ëª¨ë“  ìš”ì²­ì€ 24ì‹œê°„ ë‚´ ì²˜ë¦¬"),
                Document(page_content="íšŒì‚¬ ì—°ë½ì²˜: ë³¸ì‚¬ 02-1234-5678, ì§€ì›íŒ€ 02-8765-4321"),
                Document(page_content="ì—…ë¬´ ì‹œê°„: í‰ì¼ ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ"),
            ]
            vectorstore = FAISS.from_documents(docs, _embeddings)
            st.write(f"âœ… ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ ({len(docs)}ê°œ ë¬¸ì„œ)")
            return vectorstore
        except Exception as e:
            st.error(f"âŒ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            raise e

    def _create_llm(self):
        """LLM ìƒì„± ë° ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            st.write("ğŸ”„ LLM ì´ˆê¸°í™” ì¤‘...")
            llm = ChatOllama(
                model=os.getenv("LLM_MODEL", "llama3"),
                base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                temperature=0.7,
                num_predict=50  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì‘ë‹µ ê¸¸ì´ ì œí•œ
            )
            
            # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸
            st.write("ğŸ”„ LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_response = llm.invoke("Hi")
            st.write(f"âœ… LLM ì—°ê²° ì„±ê³µ (ì‘ë‹µ: {test_response.content[:30]}...)")
            return llm
            
        except Exception as e:
            error_msg = str(e)
            if "connection" in error_msg.lower() or "timeout" in error_msg.lower():
                st.error("âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            elif "model" in error_msg.lower():
                st.error(f"âŒ ëª¨ë¸ '{os.getenv('LLM_MODEL', 'llama3')}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
            else:
                st.error(f"âŒ LLM ì—°ê²° ì‹¤íŒ¨: {error_msg}")
            raise e

    def build_base_chain(self):
        prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ì „ë¬¸ ê°œì¸ ë¹„ì„œì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
            1. ì‚¬ìš©ì ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ ì œê³µ
            2. ì¹œì ˆí•˜ê³  ì •ì¤‘í•œ ë§íˆ¬ ì‚¬ìš© (ì¡´ëŒ“ë§)
            3. ë‹µë³€ì€ ê°„ê²°í•˜ë˜ í•„ìš”í•œ ì •ë³´ëŠ” ì¶©ë¶„íˆ í¬í•¨
            4. ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€

            ëŒ€í™” ì´ë ¥: {history}
            í˜„ì¬ ìš”ì²­: {message}
            
            ë‹µë³€:"""
        )
        return prompt | self.llm | StrOutputParser()

    def build_rag_chain(self):
        rag_prompt = ChatPromptTemplate.from_template(
            """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

            ì°¸ê³  ë¬¸ì„œ:
            {context}

            ì§ˆë¬¸: {question}
            
            ë‹µë³€: ì°¸ê³  ë¬¸ì„œì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."""
        )
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        return (
            {"context": self.retriever | format_docs, "question": RunnablePassthrough()}
            | rag_prompt
            | self.llm
            | StrOutputParser()
        )

    def build_conversation_chain(self):
        # ìˆ˜ì •: ëŒ€í™” ì²´ì¸ì„ ë‹¨ìˆœí™”
        base_chain = self.build_base_chain()
        
        def conversation_handler(message: str) -> Dict[str, Any]:
            """ëŒ€í™” ì²˜ë¦¬ í•¨ìˆ˜"""
            # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
            if 'conversation_history' not in st.session_state:
                st.session_state.conversation_history = []
            
            history = st.session_state.conversation_history
            
            # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
            formatted_history = "\n".join([
                f"ì‚¬ìš©ì: {conv['user']}\në¹„ì„œ: {conv['assistant']}"
                for conv in history[-3:]
            ])
            
            try:
                response = base_chain.invoke({
                    "message": message,
                    "history": formatted_history
                })
                
                # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                history.append({"user": message, "assistant": response})
                
                # ìµœëŒ€ 10ê°œ ëŒ€í™”ë§Œ ìœ ì§€
                if len(history) > 10:
                    history.pop(0)
                
                return {"response": response, "success": True}
                
            except Exception as e:
                error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                return {"response": error_msg, "success": False}
        
        return conversation_handler